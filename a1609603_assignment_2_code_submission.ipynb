{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a1609603_assignment_2_code_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO49Jefxu/Adua7aZv48SYf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4746d280f3eb4044ab67968cd35f5473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_85b2b32d8f3e41fc93645d31e8136b33",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8229b590994c49dbbefeed8f654a5f12",
              "IPY_MODEL_bbd61284bb5342eabea2ed22304c8d5d",
              "IPY_MODEL_21f1c9bfe10548f98d8797521f6fb61e"
            ]
          }
        },
        "85b2b32d8f3e41fc93645d31e8136b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8229b590994c49dbbefeed8f654a5f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a2f580859ae48f399bbbf8e3284cffa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4f0d132db3849f08b225e7242c22a11"
          }
        },
        "bbd61284bb5342eabea2ed22304c8d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_480f3fcbaaed4907978d1789f4b689c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fc4d69c044d4eef8249b8223e060874"
          }
        },
        "21f1c9bfe10548f98d8797521f6fb61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12945a1e84944a628fddbf64d3b21859",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 58681519.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_feabc1408b714796b3553b01009b7011"
          }
        },
        "1a2f580859ae48f399bbbf8e3284cffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4f0d132db3849f08b225e7242c22a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "480f3fcbaaed4907978d1789f4b689c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fc4d69c044d4eef8249b8223e060874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12945a1e84944a628fddbf64d3b21859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "feabc1408b714796b3553b01009b7011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pebtL2ZpswKa"
      },
      "source": [
        "Author: Pablo Nicolas Blanco\n",
        "\n",
        "Student ID: a1609603\n",
        "\n",
        "ASSIGNMENT 2 - Deep Learning Fundamentals\n",
        "\n",
        "Description: Using Pytorch, this program implements a residual CNN for deep learning of the CIFAR 10 image data set. In this program, the depths for both convolutional and dense layers can be increased using the variables addResidualBlocksFlag, and extraDeepLayerFlag in the forward function of the Net class. The mini-batch size can also be specified in the data-loading code cell. The Adam optimiser with default parameters is used to train the network, together with a cross entropy loss. During training, the training and validation accuracy are printed out for each epoch. The network with parameters that achieve the highest validation accuracy is saved.  The trained network is then applied to a test data set, and test accuracies for the image classes are reported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4_N2JrSsuiN"
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2zBExnHfKD8"
      },
      "source": [
        "Load in data, convert to tensor, apply normalisation and set the mini batch size. Training and testing data sets are downloaded separately. The training set is further split into two sets; a training set and a validation set (using an 80%/20% random split)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp2ZWefTTnHy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "4746d280f3eb4044ab67968cd35f5473",
            "85b2b32d8f3e41fc93645d31e8136b33",
            "8229b590994c49dbbefeed8f654a5f12",
            "bbd61284bb5342eabea2ed22304c8d5d",
            "21f1c9bfe10548f98d8797521f6fb61e",
            "1a2f580859ae48f399bbbf8e3284cffa",
            "e4f0d132db3849f08b225e7242c22a11",
            "480f3fcbaaed4907978d1789f4b689c1",
            "7fc4d69c044d4eef8249b8223e060874",
            "12945a1e84944a628fddbf64d3b21859",
            "feabc1408b714796b3553b01009b7011"
          ]
        },
        "outputId": "3d1a24ad-d3ab-4857-f4c6-eac293cfb9cb"
      },
      "source": [
        "# set the mini_batch size, the same batch size is to be used for the training, validation and test data\n",
        "miniBatchSize = 10\n",
        "\n",
        "# all data is to be converted to tensor data structure as well as normalised\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # mean of 0.5 and standard deviation of 0.5 for all three rgb channels\n",
        "\n",
        "# load the training set (50000 images)\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# put aside 20% of the original training set for validation during training, specify a pseudo-random number generation seed so that the validation set is always the same\n",
        "trainset, validset = torch.utils.data.random_split(trainset,[40000,10000],generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# put training data into a data loader, which is composed of mini-batches\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=miniBatchSize,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# put validation data into a data loader, which is composed of mini-batches\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=miniBatchSize,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "# load the test set (10000 images)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# put test data into a data loader, which is composed of mini-batches\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=miniBatchSize,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# class names in the order of the integer labels for them, i.e. 0 is airplane, 1 is automobile, 2 is bird, etc\n",
        "class_names = ('airplane', 'automobile', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4746d280f3eb4044ab67968cd35f5473",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B655UNTVpEEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf08c998-a815-4136-eae4-22110a8f353d"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "      \n",
        "        # batch normalisation, just the the number of channels\n",
        "        # input to Conv2D is the number of channels, and the output is the number of filters (there are as many feature maps/channels generated as there are filters)\n",
        "  \n",
        "        self.conv6_kernel7_change = nn.Conv2d(3, 6, kernel_size=7, padding=3)  # change from 3 channels/feature maps to 6 channels/feature maps by applying 6 filters\n",
        "        \n",
        "        self.bn6_0 = nn.BatchNorm2d(6)\n",
        "        self.bn6_1 = nn.BatchNorm2d(6)\n",
        "        self.bn6_2 = nn.BatchNorm2d(6)\n",
        "        self.bn6_3 = nn.BatchNorm2d(6)\n",
        "        self.bn6_4 = nn.BatchNorm2d(6)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2) # window of size 2, applied with stride 2, this will reduce the image size by half (from 32x32 to 16x16)\n",
        "\n",
        "        self.conv6_1 = nn.Conv2d(6, 6, kernel_size=3, padding=1)\n",
        "        self.conv6_2 = nn.Conv2d(6, 6, kernel_size=3, padding=1)\n",
        "        self.conv6_3 = nn.Conv2d(6, 6, kernel_size=3, padding=1)\n",
        "        self.conv6_4 = nn.Conv2d(6, 6, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.conv16_change = nn.Conv2d(6, 16, kernel_size=3, padding=1)\n",
        "        self.conv16_change_in_skip = nn.Conv2d(6, 16, kernel_size=1)\n",
        "        \n",
        "        self.conv16_1 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.conv16_2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.conv16_3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.bn16_skip = nn.BatchNorm2d(16)\n",
        "        self.bn16_0 = nn.BatchNorm2d(16)\n",
        "        self.bn16_1 = nn.BatchNorm2d(16)\n",
        "        self.bn16_2 = nn.BatchNorm2d(16)\n",
        "        self.bn16_3 = nn.BatchNorm2d(16)\n",
        "        self.bn16_4 = nn.BatchNorm2d(16)\n",
        "                \n",
        "        self.convDownsizeWithKernel7 = nn.Conv2d(16, 16, kernel_size=7) # has no padding, therefore will reduce feature map sizes by 6\n",
        "        \n",
        "        self.fullyConn1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fullyConn2 = nn.Linear(120, 84)\n",
        "        self.fullyConn3 = nn.Linear(84, 10)\n",
        "\n",
        "        self.fullyConn2_alternative = nn.Linear(120,10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the adjustable depth flags of the network\n",
        "        addResidualBlocksFlag = False\n",
        "        extraDeepLayerFlag = True\n",
        "\n",
        "        # convolution with kernel size 7\n",
        "        x = self.conv6_kernel7_change(x)\n",
        "        x = self.bn6_0(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # make it 16x16 with max pooling\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # FIRST CONV 6 RESIDUAL BLOCK (composed of two convolutions, implementation of channel quantity change not needed since the change from 3 to 6 feature maps is done in previous conv layer)\n",
        "        # make a residual to be used as a skip connection\n",
        "        residual = x\n",
        "        x = self.conv6_1(x)\n",
        "        x = self.bn6_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6_2(x)\n",
        "        x = self.bn6_2(x)\n",
        "        x = x + residual  # the skip connection almost skips two convolutional layers, i.e. it is put just before the relu of the second convolutional layer\n",
        "        x = F.relu(x) \n",
        "\n",
        "        if (addResidualBlocksFlag):\n",
        "          residual = x\n",
        "          x = self.conv6_3(x)\n",
        "          x = self.bn6_3(x)\n",
        "          x = F.relu(x)\n",
        "          x = self.conv6_4(x)\n",
        "          x = self.bn6_4(x)\n",
        "          x = x + residual  # the skip connection almost skips two convolutional layers, i.e. it is put just before the relu of the second convolutional layer\n",
        "          x = F.relu(x) \n",
        "\n",
        "        # FIRST CONV 16 RESIDUAL BLOCK (composed of two convolutions, implementation of channel quantity change is needed both in first convolution or in a 1x1 convolution in the skip connection)\n",
        "        residual = x\n",
        "        x = self.conv16_change(x)\n",
        "        x = self.bn16_0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv16_1(x)\n",
        "        x = self.bn16_1(x)\n",
        "        x = x + self.bn16_skip(self.conv16_change_in_skip(residual)) # 1x1 convolutional layer implements channel quantity change in skip connection, since later layers expect 16 feature maps\n",
        "        x = F.relu(x)\n",
        "\n",
        "        if (addResidualBlocksFlag):\n",
        "          residual = x\n",
        "          x = self.conv16_2(x)\n",
        "          x = self.bn16_2(x)\n",
        "          x = F.relu(x)\n",
        "          x = self.conv16_3(x)\n",
        "          x = self.bn16_3(x)\n",
        "          x = x + residual  # the skip connection almost skips two convolutional layers, i.e. it is put just before the relu of the second convolutional layer\n",
        "          x = F.relu(x) \n",
        "\n",
        "\n",
        "        # Reduce feature map size from 16x16 to 10x10 with a kernel of 7 without padding\n",
        "        x = self.convDownsizeWithKernel7(x)\n",
        "        x = self.bn16_4(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Reduce feature map size from 10x10 to 5x5 with max pooling\n",
        "        x = self.pool(x)\n",
        "       \n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fullyConn1(x))\n",
        "        if (extraDeepLayerFlag):\n",
        "          x = F.relu(self.fullyConn2(x))\n",
        "          x = self.fullyConn3(x) # output of last dense layer goes to the softmax activatino function, hence relu is not applied\n",
        "        else:\n",
        "          x = self.fullyConn2_alternative(x) # output of last dense layer goes to the softmax activatino function, hence relu is not applied\n",
        "        return x\n",
        "\n",
        "# instantiate the network that will be trained\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv6_kernel7_change): Conv2d(3, 6, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "  (bn6_0): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6_1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6_2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6_3): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6_4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv6_1): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv6_2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv6_3): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv6_4): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv16_change): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv16_change_in_skip): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv16_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv16_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv16_3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn16_skip): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn16_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn16_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn16_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn16_3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn16_4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (convDownsizeWithKernel7): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1))\n",
            "  (fullyConn1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fullyConn2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fullyConn3): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (fullyConn2_alternative): Linear(in_features=120, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmvtkYKcQl_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56cf094d-9e2d-415f-d561-ee2e1b03d91b"
      },
      "source": [
        "# define the device, try to get a CUDA machine if available, if not then use cpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "# assign network to device\n",
        "net.to(device)\n",
        "print(net.conv6_1.bias.get_device())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xzbengRDpG"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0)\n",
        "optimizer = torch.optim.Adam(net.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uIOlAIqRMwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7c7cd7-4ab9-4f97-b425-5ff3d84c13d2"
      },
      "source": [
        "# store epochs, and the training and validation accuracies for each opech in these lists, these are printed and used later for postprocessing\n",
        "epochs_list = []\n",
        "train_accuracy_list = []\n",
        "valid_accuracy_list = []\n",
        "\n",
        "# total number of epochs used to train the model\n",
        "total_epochs = 20\n",
        "# variable to store the maximum validation accuracy, initialise as zero\n",
        "max_valid_accuracy = 0.0\n",
        "\n",
        "# save the network to this path\n",
        "PATH = './saved_net.pth'\n",
        "\n",
        "# iterate over the specified number of epochs\n",
        "for epoch in range(total_epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "    train_total = 0.0\n",
        "    train_correct = 0.0\n",
        "\n",
        "    net.train() # this is needed because batch norm layers are present in the network, and they are meant to behave differently in training than they do in testing/evaluation \n",
        "    # iterate over all the mini batches that make up one epoch (i.e. the whole training set), and assign variable i to keep count of mini-batches \n",
        "    for i, mini_batch_data in enumerate(trainloader, 0):\n",
        "        # mini_batch_data is a list of [inputs, labels], separate these into different objects, and send them to the GPU at every step\n",
        "        inputs, labels = mini_batch_data[0].to(device), mini_batch_data[1].to(device)\n",
        "\n",
        "        # zero the loss gradients with respect to parameters\n",
        "        optimizer.zero_grad()\n",
        "        # pass the inputs forward through the network, and obtain the outputs used for prediction\n",
        "        outputs = net(inputs)\n",
        "        # average loss for the mini-batch (which would be a specified number of images, as specified in the transform)\n",
        "        loss = criterion(outputs, labels)   \n",
        "        # backpropagation to calculate the partial gradients of the loss with respect to each parameter\n",
        "        loss.backward()\n",
        "        # optimize the parameters in the network by updating them based on the loss gradient with respect to a parameter, and the learning rate\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep counts of total samples, and of correctly predicted samples\n",
        "        a, pred = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += pred.eq(labels).sum().item()\n",
        "\n",
        "    valid_total = 0.0\n",
        "    valid_correct = 0.0\n",
        "\n",
        "    net.eval() # this is needed because batch norm layers are present in the network, and they are meant to behave differently in training than they do in testing/evaluation \n",
        "    # iterate over all the mini batches in the validation set\n",
        "    for j, mini_batch_data in enumerate(validloader, 0):\n",
        "        # mini_batch_data is a list of [inputs, labels], separate these into different objects, and send them to the GPU at every step\n",
        "        inputs_valid, labels_valid = mini_batch_data[0].to(device), mini_batch_data[1].to(device)\n",
        "\n",
        "        # pass the inputs forward through the network, and obtain the outputs used for prediction\n",
        "        outputs_valid = net(inputs_valid)\n",
        "       \n",
        "        # keep counts of total samples, and of correctly predicted samples\n",
        "        a, pred = outputs_valid.max(1)\n",
        "        valid_total += labels_valid.size(0)\n",
        "        valid_correct += pred.eq(labels_valid).sum().item()\n",
        "\n",
        "    epoch_num = epoch + 1\n",
        "    train_accuracy = 100*train_correct/train_total # divide the sum of the mini-batch losses by the number of mini-batches, this gives the average loss for an epoch, i.e. the empirical risk\n",
        "    valid_accuracy = 100*valid_correct/valid_total\n",
        "\n",
        "    print('Epoch: %d  Train Accuracy: %.1f %%  Validation Accuracy: %.1f %%' %\n",
        "                  (epoch_num, train_accuracy , valid_accuracy))\n",
        "\n",
        "    epochs_list.append(epoch_num)\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    valid_accuracy_list.append(valid_accuracy)\n",
        "\n",
        "    # if there is an increase in validation accuracy, save the network\n",
        "    if valid_accuracy > max_valid_accuracy:\n",
        "      max_valid_accuracy = valid_accuracy\n",
        "      torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "print('The values in list form are printed below:')\n",
        "print(epochs_list)\n",
        "print(train_accuracy_list)\n",
        "print(valid_accuracy_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1  Train Accuracy: 43.1 %  Validation Accuracy: 54.6 %\n",
            "Epoch: 2  Train Accuracy: 56.0 %  Validation Accuracy: 59.9 %\n",
            "Epoch: 3  Train Accuracy: 61.6 %  Validation Accuracy: 62.0 %\n",
            "Epoch: 4  Train Accuracy: 64.9 %  Validation Accuracy: 66.0 %\n",
            "Epoch: 5  Train Accuracy: 67.7 %  Validation Accuracy: 65.0 %\n",
            "Epoch: 6  Train Accuracy: 69.7 %  Validation Accuracy: 66.4 %\n",
            "Epoch: 7  Train Accuracy: 71.1 %  Validation Accuracy: 67.2 %\n",
            "Epoch: 8  Train Accuracy: 72.4 %  Validation Accuracy: 68.0 %\n",
            "Epoch: 9  Train Accuracy: 73.7 %  Validation Accuracy: 67.9 %\n",
            "Epoch: 10  Train Accuracy: 74.9 %  Validation Accuracy: 66.9 %\n",
            "Epoch: 11  Train Accuracy: 75.8 %  Validation Accuracy: 68.1 %\n",
            "Epoch: 12  Train Accuracy: 76.7 %  Validation Accuracy: 67.7 %\n",
            "Epoch: 13  Train Accuracy: 77.7 %  Validation Accuracy: 67.8 %\n",
            "Epoch: 14  Train Accuracy: 78.4 %  Validation Accuracy: 67.4 %\n",
            "Epoch: 15  Train Accuracy: 79.2 %  Validation Accuracy: 66.3 %\n",
            "Epoch: 16  Train Accuracy: 79.7 %  Validation Accuracy: 66.9 %\n",
            "Epoch: 17  Train Accuracy: 80.7 %  Validation Accuracy: 67.2 %\n",
            "Epoch: 18  Train Accuracy: 81.1 %  Validation Accuracy: 67.2 %\n",
            "Epoch: 19  Train Accuracy: 81.9 %  Validation Accuracy: 66.8 %\n",
            "Epoch: 20  Train Accuracy: 82.1 %  Validation Accuracy: 66.7 %\n",
            "Training Completed\n",
            "The values in list form are printed below:\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "[43.0575, 55.9775, 61.6075, 64.9425, 67.7, 69.7175, 71.115, 72.3525, 73.67, 74.86, 75.805, 76.6575, 77.67, 78.4375, 79.2325, 79.7175, 80.6925, 81.1025, 81.88, 82.0675]\n",
            "[54.57, 59.89, 61.99, 65.97, 64.97, 66.37, 67.21, 67.96, 67.86, 66.93, 68.14, 67.73, 67.81, 67.4, 66.35, 66.88, 67.16, 67.16, 66.82, 66.74]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r722x2eoUXOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf121a3d-b1c7-4a0a-e721-75ada4f48c0a"
      },
      "source": [
        "# initialise a network and then load the saved model from training\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# set the network to testing mode\n",
        "net.eval()\n",
        "\n",
        "# calculate the accuracy for each class, and also the average accuracy\n",
        "\n",
        "correctly_predicted_counts = [0,0,0,0,0,0,0,0,0,0]\n",
        "total_counts = [0,0,0,0,0,0,0,0,0,0]\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        # obtain the images (inputs) and the correct classes (labels) for the mini-batch\n",
        "        inputs, labels = data\n",
        "        # pass the images  of the mini-batch through the network and obtain the softmax output\n",
        "        outputs = net(inputs)\n",
        "        # obtain the predicted classes for this mini-batch by, for each sample, selecting the class with the highest score\n",
        "        a, pred = torch.max(outputs, 1)\n",
        "        # iterate through the samples in the mini-batch\n",
        "        for i in range(miniBatchSize):\n",
        "            # the integer class labels are used as indices for assigning counts to the lists that keep count of the correct and total samples for each class\n",
        "            # if the prediction matched the label, add to the correct count\n",
        "            if (pred[i] == labels[i]):\n",
        "              correctly_predicted_counts[labels[i]] += 1\n",
        "            total_counts[labels[i]] += 1\n",
        "\n",
        "# print individual class accuracies and the average accuracy\n",
        "\n",
        "average_accuracy = 0\n",
        "for i in range(10):\n",
        "    accuracy =  100 * correctly_predicted_counts[i] / total_counts[i]\n",
        "    average_accuracy += accuracy\n",
        "    print('Accuracy of %5s : %.1f %%' % (\n",
        "        class_names[i], accuracy ))\n",
        "    \n",
        "average_accuracy = average_accuracy / 10\n",
        "print('Average accuracy: %.1f %%' % (\n",
        "        average_accuracy ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of airplane : 73.5 %\n",
            "Accuracy of automobile : 81.5 %\n",
            "Accuracy of  bird : 52.4 %\n",
            "Accuracy of   cat : 50.0 %\n",
            "Accuracy of  deer : 55.6 %\n",
            "Accuracy of   dog : 56.1 %\n",
            "Accuracy of  frog : 78.7 %\n",
            "Accuracy of horse : 76.5 %\n",
            "Accuracy of  ship : 74.2 %\n",
            "Accuracy of truck : 80.4 %\n",
            "Average accuracy: 67.9 %\n"
          ]
        }
      ]
    }
  ]
}